import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pingouin as pg
import seaborn as sns
import shap
import string
import time
from azureml.core import Run
from catboost import CatBoostRegressor
from imblearn.over_sampling import SMOTE
from lightgbm import LGBMRegressor
from pathos.helpers import cpu_count
from pathos.pools import ProcessPool
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import cross_val_predict, train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor, StackingRegressor, VotingRegressor
from xgboost import XGBRegressor


class Regressor:
        
    dict_regressors = {
        "LinR": LinearRegression(),
        "RidgeR": Ridge(random_state=42),
        "Lasso": Lasso(random_state=42),
        "ENet": ElasticNet(random_state=42),
        "KRR": KernelRidge(),
        "AdaR": AdaBoostRegressor(random_state=42),
        "GBR": GradientBoostingRegressor(random_state=42),
        "XGBR": XGBRegressor(random_state=42),
        "LGBMR": LGBMRegressor(random_state=42),
        "BaggingR": BaggingRegressor(random_state=42),
        "SVR": SVR(), # kernel == 'poly' | 'linear' | 'sigmoid'
        "KNR": KNeighborsRegressor(),
        "DTR": DecisionTreeRegressor(random_state=42),
        "RFR": RandomForestRegressor(random_state=42),
        "ExtraR": ExtraTreesRegressor(random_state=42),
        "CatBR": CatBoostRegressor(silent=True, random_state=42)
    }

    def __init__(self, data, keep_cols, target, transform=None, ref_col=None, online=False):
        """ construction of Regressor class 
        """
        if type(target) == list:
            self.data = data[keep_cols + sorted(list(set(data.columns.tolist()) - set(keep_cols + target))) + target]
            self.problem = "multi_output"
        else:
            self.data = data[keep_cols + sorted(list(set(data.columns.tolist()) - set(keep_cols + [target]))) + [target]]
            self.problem = "single_output"
        self.target = target
        self.transform = transform
        self.ref_col = ref_col
        self.keep_cols = keep_cols
        self.split_x_and_y()
        if self.problem == "single_output":
            print("Regressor initialized")
        else:
            print("Multi-output regressor initialized")
        self.online_run = online
        if self.online_run:
            self.run = Run.get_context()

    ### TRAIN / TEST SPLIT ###
    def split_x_and_y(self):
        """ split target from the data 
        """
        if self.problem == "multi_output":
            self.features = sorted(list(set(self.data.columns.tolist()) - set(self.keep_cols + self.target)))
        else:
            self.features = sorted(list(set(self.data.columns.tolist()) - set(self.keep_cols + [self.target])))
        if len(self.features) <= 25:
            print("Training will be done using the following features:\n", self.features)
        self.X = self.data[self.features].copy()
        self.y = self.data[self.target].copy()
        print("Data is split into X and y:\n",
                "\tX:", self.X.shape, "\n",
                "\ty:", self.y.shape)

    def generate_train_test(self, test_size=0.25):
        """ create train test sets for modeling
        """
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=42, shuffle=True)
        print("Train data size:", self.X_train.shape)
        print("Test data size:", self.X_test.shape)

    ### OVERSAMPLING / DATA AUGMENTATION ###
    def oversampling(self, k):
        """ oversampling method for imbalanced data
        """
        if hasattr(self, 'X_train'): 
            over = SMOTE(k_neighbors=k, random_state=42)
            self.X_train, self.y_train = over.fit_resample(self.X_train, self.y_train)
            self.X = self.X_train.copy()
            self.y = self.y_train.copy()
            print("After oversampling:\n",
                  "\tX_train:", self.X_train.shape, "\n",
                  "\ty_train:", self.y_train.shape, "\n")            
        else:
            raise AssertionError("Please first generate train & test datasets out of given data!")

    def randomized_smoothing(self, noise, extend, rounding_cols):
        """ augment training data with synthetic data generated by
            realistic gaussian noise to make modeling more robust
        """
        if hasattr(self, 'X_train'):             
            # syntetic data
            df_train = pd.concat([self.X_train, self.y_train], axis=1)
            df_train_syntetic = pd.concat([df_train]*extend)
            df_train_syntetic = np.random.normal(df_train_syntetic, df_train.std()*noise, size = df_train_syntetic.shape)
            df_train_syntetic = pd.DataFrame(data=df_train_syntetic, columns=df_train.columns)

            # postprocessing
            non_zero_cols = ((df_train < 0).any() == False).index.tolist()
            for i in non_zero_cols:
                df_train_syntetic.loc[df_train_syntetic[i] < 0, i] = 0
                
            binary_cols = df_train.columns[df_train.isin([0,1]).all()].tolist()
            for i in binary_cols:
                df_train_syntetic[i] = df_train_syntetic[i].round()
                df_train_syntetic.loc[df_train_syntetic[i] > 1, i] = 1

            for i in rounding_cols:
                df_train_syntetic[i] = df_train_syntetic[i].round()

            # concat syntetic data into original data
            df_train = pd.concat([df_train, df_train_syntetic]).reset_index(drop=True)
            self.X_train = df_train[self.features].copy()
            self.y_train = df_train[self.target].copy()
            self.X = self.X_train.copy()
            self.y = self.y_train.copy()

            print("After randomized smoothing:\n",
                  "\tTrain data size:", self.X_train.shape, "\n",
                  "\tTrain target distribution:\n", self.y_train.shape, "\n")   
        else:
            raise AssertionError("Please first generate train & test datasets out of given data!")

    ### SCORE MODELS ###
    def experiment_models(self, by_test=True, cv=5):
        """ check model performances with parallel computing
        """
        if by_test:            
            if not hasattr(self, 'X_train'): 
                self.generate_train_test() 
        cores = cpu_count()
        pool = ProcessPool(cores)
        model_names = list(self.dict_regressors.keys())
        models = list(self.dict_regressors.values())
        # experiment bunch of regression models
        print(f"Running models parallel with {cores} cores:", model_names)
        n_models = len(models)
        try:
            if by_test == True:
                scores_data = pool.amap(self.score_in_test, models, model_names)
            else:
                scores_data = pool.amap(self.cv_score_model, models, model_names, [cv] * n_models)
            while not scores_data.ready():
                time.sleep(5); print(".", end=' ')
            scores_data = scores_data.get()
        except Exception as e:
            print(f"\nCouldn't run parallel because of the following exception:", e)
            scores_data = []
            for m_name, model in self.dict_regressors.items():
                if by_test == True:
                    scores_data.append(self.score_in_test(model, m_name))
                else:
                    scores_data.append(self.cv_score_model(model, m_name, cv))      
        df_scores = pd.DataFrame(scores_data)     
        # sort score dataframe by R2
        df_scores.sort_values('R2', ascending=False, inplace=True)
        # best models => base models for stacking
        self.base_models = [(df_scores.iloc[0].model, self.dict_regressors[df_scores.iloc[0].model]),
                            (df_scores.iloc[1].model, self.dict_regressors[df_scores.iloc[1].model]),
                            (df_scores.iloc[2].model, self.dict_regressors[df_scores.iloc[2].model])]
        # set best model
        self.best_model = self.base_models[0]
        return df_scores

    def cv_score_model(self, model=None, model_name="", cv=5):
        """ do a cross validation scoring with given model if no 
            model is given then a logistic regression will be tried
        """
        if model is None:
            if hasattr(self, 'model'): 
                model = self.model
            else:
                raise AssertionError("Please pass over a model to proceed!")
        elif model == "linr":
            model = LinearRegression()
        elif model == "best":
            model = self.best_model
        elif model == "stack":
            model = self.stacking_model()
        elif model == "vote":
            model = self.voting_model()
        if model_name == "":
            model_name = extract_model_name(model)
        if self.problem == "multi_output":
            model = MultiOutputRegressor(model)
        self.pred_test = cross_val_predict(model, self.X, self.y, cv=cv) 
        self.model = model
        y = self.y.copy()
        pred_test = self.pred_test.copy()
        if self.transform == "ratio":            
            pred_test = self.X[self.ref_col] / pred_test
            y = self.X[self.ref_col] / y
        elif self.transform == "log": 
            pred_test = np.exp(pred_test)
            y = np.exp(y)           
        scores = regression_metrics(y, pred_test, model_name)
        return scores    

    def score_in_test(self, model=None, model_name=""):
        """ score the given regression model on the generated test data
        """
        if hasattr(self, 'X_train'):
            if model is None:
                if hasattr(self, 'model'): 
                    model = self.model
                else:
                    raise AssertionError("Please pass over a model to proceed!")
            elif model == "linr":
                model = LinearRegression()
            elif model == "best":
                model = self.best_model
            elif model == "stack":
                model = self.stacking_model()
            elif model == "vote":
                model = self.voting_model()
            if model_name == "":
                model_name = extract_model_name(model)
            if self.problem == "multi_output":
                model = MultiOutputRegressor(model)
            model.fit(self.X_train, self.y_train)
            self.pred_test = model.predict(self.X_test)
            self.model = model
            y_test = self.y_test.copy()
            pred_test = self.pred_test.copy()
            if self.transform == "ratio":          
                pred_test = self.X_test[self.ref_col] / pred_test
                y_test = self.X_test[self.ref_col] / y_test
            elif self.transform == "log": 
                pred_test = np.exp(pred_test)
                y_test = np.exp(y_test) 
            scores = regression_metrics(y_test, pred_test, model_name)
            return scores             
        else:
            raise AssertionError("Please first generate train & test datasets out of given data!")

    def scores_for_multi_output(self, by_test=False):
        """ score multi-output regression predictions individually
        """
        if hasattr(self, 'pred_test'): 
            if self.problem == "multi_output":
                y = self.y.copy()
                pred_test = self.pred_test.copy()
                if by_test:
                    if hasattr(self, 'X_train'):
                        y = self.y_test.copy()
                        if self.transform == "ratio": 
                            for i in range(self.y.shape[1]):
                                pred_test[:,i] = self.X_test[self.ref_col] / pred_test[:,i] 
                                y.iloc[:,i] = self.X_test[self.ref_col] / y.iloc[:,i]
                        elif self.transform == "log": 
                            pred_test = np.exp(pred_test)
                            y = np.exp(y) 
                    else:
                        raise AssertionError("Model predictions for the test data is needed first!")
                else:
                    if self.transform == "ratio":      
                        for i in range(self.y.shape[1]):
                            pred_test[:,i] = self.X[self.ref_col] / pred_test[:,i]  
                            y.iloc[:,i] = self.X[self.ref_col] / y.iloc[:,i]
                    elif self.transform == "log": 
                        pred_test = np.exp(pred_test)
                        y = np.exp(y)                    
                return {y.columns[i]: regression_metrics(y.iloc[:,i], pred_test[:,i], extract_model_name(self.model)) for i in range(self.y.shape[1])}
            else:
                raise AssertionError("This function only works when using multi-output regression models!")
        else:
            raise AssertionError("Please first have predictions of a multi-ouput regression model!")

    def calculate_errors(self, percentage_errors = [5, 10, 15, 25, 50], verbose=False):
        """ calculate residuals, errors and percentage errors of the 
            model predictions
        """
        if hasattr(self, 'pred_test'): 
            df_residuals = self.data.copy()
            if self.problem == "single_output":
                df_residuals = df_residuals.rename(columns={self.target: "actual"})
                df_residuals["prediction"] = self.pred_test   
                # do transformation
                if self.transform == "ratio":          
                    df_residuals["prediction"] = df_residuals[self.ref_col] / self.pred_test
                    df_residuals["actual"] = df_residuals[self.ref_col] / self.y
                elif self.transform == "log": 
                    df_residuals["prediction"] = np.exp(self.pred_test)
                    df_residuals["actual"] = np.exp(self.y) 
                # calculate errors
                df_residuals["error"] = df_residuals["prediction"] - df_residuals["actual"]
                df_residuals["percentage_error"] = df_residuals["error"]/df_residuals["actual"] 
                df_residuals["abs_percentage_error"] = round(100*(np.absolute(df_residuals["percentage_error"])), 1) 
                df_residuals["ratio"] = df_residuals["prediction"] / df_residuals["actual"]
                df_residuals["evaluation"] = df_residuals["abs_percentage_error"].apply(evaluate)
                if verbose:
                    print("Number of observations per percentage error:")
                    [print(f'\t<={val}%:', df_residuals[df_residuals["abs_percentage_error"] <= val].shape[0]) for val in percentage_errors]
            elif self.problem == "multi_output":
                for i in range(self.y.shape[1]):
                    df_residuals[f"actual_{i}"] = self.y.iloc[:,i] 
                    df_residuals[f"prediction_{i}"] = self.pred_test[:,i]
                df_residuals.drop(columns=self.target, inplace=True)
                # do transformation
                target_cols = [i for i in df_residuals.columns if i.startswith("actual_") | i.startswith("prediction_")]        
                if self.transform == "ratio": 
                    for col_i in target_cols:
                        df_residuals[col_i] = self.X[self.ref_col] / df_residuals[col_i]
                elif self.transform == "log": 
                    for col_i in target_cols:
                        df_residuals[col_i] = np.exp(df_residuals[col_i])
                # calculate errors
                for i in range(self.y.shape[1]): 
                    df_residuals[f"error_{i}"] = df_residuals[f"prediction_{i}"] - df_residuals[f"actual_{i}"]
                    df_residuals[f"percentage_error_{i}"] = df_residuals[f"error_{i}"]/df_residuals[f"actual_{i}"] 
                    df_residuals[f"abs_percentage_error_{i}"] = round(100*(np.absolute(df_residuals[f"percentage_error_{i}"])), 1) 
                    df_residuals[f"ratio_{i}"] = df_residuals[f"prediction_{i}"] / df_residuals[f"actual_{i}"]
                    df_residuals[f"evaluation_{i}"] = df_residuals[f"abs_percentage_error_{i}"].apply(evaluate)
                if verbose:
                    print("Number of observations per percentage error:")
                    for i in range(self.y.shape[1]):
                        [print(f'\t<={val}%:', df_residuals[df_residuals[f"abs_percentage_error_{i}"] <= val].shape[0]) for val in percentage_errors]
            return df_residuals
        else:
            raise AssertionError("Please first have predictions to check the distribution and scoring metrics!")

    ### ENSEMBL MODELS ### 
    def define_base_models(self, base_list):
        """ give a list of model abbreviations to be used
            in stacking
        """
        self.base_models = []
        for name in base_list:
            self.base_models.append((name, self.dict_regressors[name]))
        print("Base models are defined!")

    def stacking_model(self):
        """ create a stacking model using best 3 base models
            that are defined after experimenting all models
        """
        if hasattr(self, 'base_models'): 
            meta_model = LinearRegression()
            final_model = StackingRegressor(estimators=self.base_models, final_estimator=meta_model, cv=5)
            return final_model
        else:
            raise AssertionError("Please first experiment models to set top 3 base models!")

    def voting_model(self):
        """ create a voting model using best 3 base models
            that are defined after experimenting all models
        """
        if hasattr(self, 'base_models'): 
            final_model = VotingRegressor(estimators=self.base_models)
            return final_model
        else:
            raise AssertionError("Please first experiment models to set top 3 base models!")

    def cv_ensemble_models(self, models):
        """ create a ensemble of different regressors manually given 
            a list of models
        """
        self.ensemble_models = models
        df_preds = pd.DataFrame(self.y).copy()
        # individual model predictions
        for model in models:
            model_name = ''.join([char for char in extract_model_name(model) if char.isupper()])
            print(model_name, "scores\t:", self.cv_score_model(model=model, model_name=model_name))
            if self.problem == "single_output":
                df_preds[f"{model_name.lower()}_pred"] = self.pred_test
            elif self.problem == "multi_output":
                for i in range(self.y.shape[1]):
                    df_preds[f"{model_name.lower()}_pred_{i}"] = self.pred_test[:,i]
        # meta model
        meta_model = LinearRegression()   
        if self.problem == "single_output":      
            pred_cols = [i for i in df_preds.columns if i.endswith("_pred")]    
            meta_model.fit(df_preds[pred_cols], df_preds[self.target])
            coefs = dict(zip(pred_cols, meta_model.coef_/meta_model.coef_.sum()))
        elif self.problem == "multi_output": 
            coefs = {}
            for i in range(self.y.shape[1]):
                pred_cols_i = [col_i for col_i in df_preds.columns if col_i.endswith(f"_pred_{i}")] 
                meta_model.fit(df_preds[pred_cols_i], self.y.iloc[:,i])
                coefs[i] = dict(zip(pred_cols_i, meta_model.coef_/meta_model.coef_.sum()))
        print("Model coefficients:")
        print(coefs)
        self.ensemble_coefficients = coefs
        # combined predicitons
        if self.problem == "single_output":  
            df_preds["final_pred"] = sum([df_preds[i]*coefs[i] for i in coefs.keys()]) 
        elif self.problem == "multi_output": 
            for i in range(self.y.shape[1]):
                df_preds[f"final_pred_{i}"] = sum([df_preds[j]*coefs[i][j] for j in coefs[i].keys()])    
        # do transformation
        pred_cols = [i for i in df_preds.columns if "_pred" in i]        
        if self.transform == "ratio": 
            for col_i in pred_cols:
                df_preds[col_i] = self.X[self.ref_col] / df_preds[col_i]
            df_preds[self.target] = self.X[self.ref_col] / df_preds[self.target]
        elif self.transform == "log": 
            for col_i in pred_cols:
                df_preds[col_i] = np.exp(df_preds[col_i])
            df_preds[self.target] = np.exp(df_preds[self.target])
        # report scores
        if self.problem == "single_output":              
            print("Ensemble scores\t:", regression_metrics(df_preds[self.target], df_preds.final_pred, model_name="Ensemble"))
        elif self.problem == "multi_output":  
            final_cols = [f"final_pred_{i}" for i in range(self.y.shape[1])]            
            #[print("Ensemble scores\t:", regression_metrics(df_preds[self.target].iloc[:,i], df_preds[f"final_pred_{i}"], model_name="Ensemble")) for i in range(self.y.shape[1])]
            print("Ensemble scores\t:", regression_metrics(df_preds[self.target], df_preds[final_cols], model_name="Ensemble"))
        return df_preds

    ### TRAIN GIVEN MODEL & PREDICT GIVEN TEST ###
    def train_model(self, model=None):
        """ train the given regression model on whole data
        """   
        if model is None:
            if hasattr(self, 'model'): 
                model = self.model
            else:
                raise AssertionError("Please pass over a model to proceed!")
        elif model == "linr":
            model = LinearRegression()
        elif model == "best":
            model = self.best_model
        elif model == "stack":
            model = self.stacking_model()
        elif model == "vote":
            model = self.voting_model()
        model.fit(self.X, self.y)
        print("Model is fit on whole data!")
        self.trained_model = model

    def predict_test(self, df):
        """ predict the given data with the trained model
            and return the same data including predictions
        """   
        # align data
        try:
            df_X = df[self.features]
        except Exception as e:
            print("Given data doesn't have the same set of features as training!")
        if hasattr(self, 'trained_model'):
            # do predictions by trained model
            preds = self.trained_model.predict(df_X)
            # transform predictions if necessary
            if self.transform == "ratio":          
                preds = self.df_X[self.ref_col] / preds
            elif self.transform == "log": 
                preds = np.exp(preds)
            # map predictions
            if self.problem == "single_output":
                df["prediction"] = preds           
            elif self.problem == "multi_output":
                for i in range(self.y.shape[1]):
                    df[f"prediction_{i}"] = preds[:,i]
            return df
        else:
            raise AssertionError("Please first train a model then predict on the test data!")

    ### EXPLAIN MODEL ###
    def explain_model_with_shap(self, model=None):
        """ show SHAP values to explain the output of the regression model
        """
        if model is None:
            if hasattr(self, 'model'): 
                if self.problem == "single_output":
                    model = self.model
                elif self.problem == "multi_output":
                    model = self.model.estimator
            else:
                raise AssertionError("Please pass over a model to proceed!")
        # single output
        if self.problem == "single_output":
            model.fit(self.X, self.y)
            explainer = shap.Explainer(model)
            shap_values = explainer(self.X)
            shap.plots.beeswarm(shap_values)
        # multi output
        elif self.problem == "multi_output":
            for i in range(self.y.shape[1]):
                print(self.target[i])
                model = model.fit(self.X, self.y.iloc[:, i])
                explainer = shap.Explainer(model)
                shap_values = explainer(self.X)
                shap.plots.beeswarm(shap_values)

    def regression_plots(self, lower_threshold=0.4, higher_threshold=2.5, res=0.05, name=''):
        """ 4 regression plots to understand the prediction vs ground-truth
            results better
        """
        df_residuals = self.calculate_errors()  
        prediction_cols = [col_i for col_i in df_residuals.columns if col_i.startswith(f"prediction")]
        actual_cols = [col_i for col_i in df_residuals.columns if col_i.startswith(f"actual")]
        error_cols = [col_i for col_i in df_residuals.columns if col_i.startswith(f"error")]
        percentage_error_cols = [col_i for col_i in df_residuals.columns if col_i.startswith(f"percentage_error")]
        ratio_cols = [col_i for col_i in df_residuals.columns if col_i.startswith(f"ratio")]
        evaluation_cols = [col_i for col_i in df_residuals.columns if col_i.startswith(f"evaluation")]
        for i in range(len(prediction_cols)):
            fig, axes = plt.subplots(2, 2, figsize=(18, 14))
            # 1. histogram distribution
            plt.figure(figsize=(24, 18))
            df_residuals_ = df_residuals[(df_residuals[ratio_cols[i]] > lower_threshold) & (df_residuals[ratio_cols[i]] < higher_threshold)]  
            sns.histplot(data=df_residuals.iloc[df_residuals_.index], x=ratio_cols[i], hue=evaluation_cols[i], binwidth=res,
                         palette={"5%":  "#205072", 
                                  "10%": "#33709c", 
                                  "15%": "#329D9C",
                                  "25%": "#56C596",
                                  "50%": "#7BE495",
                                  "off": "#CFF4D2"}, hue_order = ["5%", "10%", "15%", "25%", "50%", "off"], ax=axes[0][0])
            axes[0][0].set(xlabel='Ratio', ylabel='Count', title='Histogram Distribution of Ratio')
            # 2. plot regression plot in colors
            palette={"5%":  "#1c3e5c",  
                     "10%": "#224b6e",
                     "15%": "#306896", 
                     "25%": "#41739c",
                     "50%": "#679bc7",
                     "off": "#caddee"}
            sns.regplot(x=df_residuals[prediction_cols[i]], y=df_residuals[actual_cols[i]], order=2, ax=axes[0][1])
            sns.scatterplot(data=df_residuals, x=prediction_cols[i], y=actual_cols[i], hue=evaluation_cols[i], palette=palette, hue_order = ["5%", "10%", "15%", "25%", "50%", "off"], ax=axes[0][1])
            axes[0][1].set(xlabel='Predicted', ylabel='Target', title='Regression Line')
            # 3. plot predictions vs residuals 
            sns.regplot(x=df_residuals[prediction_cols[i]], y=df_residuals[error_cols[i]], color='b', order=2, ax=axes[1][0])
            axes[1][0].set(xlabel='Predicted', ylabel='Residual', title='Predictions vs Residuals')
            # 4. plot qq plot of residuals
            pg.qqplot(df_residuals[error_cols[i]], dist='norm', ax=axes[1][1])
            axes[1][1].set(title='QQ-Plot of Residuals')
            # 5. plot predictions vs percentage error
            #sns.regplot(x=df_residuals[prediction_cols[i]], y=df_residuals[percentage_error_cols[i]], color='b', order=2, ax=axes[2][0])
            #axes[2][0].set(xlabel='Predicted', ylabel='Ratio Residual', title='Predictions vs Ratio Residuals')
            # 6. plot qq plot of percentage error
            #pg.qqplot(df_residuals[percentage_error_cols[i]], dist='norm', ax=axes[2][1])
            #axes[2][1].set(title='QQ-Plot of Ratio Residuals')
            if len(prediction_cols) == 1:
                fig.suptitle(f"Prediction vs {self.target}", fontsize=16)
            else:
                fig.suptitle(f"Prediction vs {self.target[i]}", fontsize=16)
            fig.tight_layout()
            plt.show()
            # save figure if on cloud
            if self.online_run:
                filename=f'./outputs/regplot_{name}_{i}.png'
                plt.savefig(filename, dpi=600)
                plt.close()

    ### QUANTILE REGRESSION ###
    def quantile_regression(self, postprocessing, threshold_interval, quantiles=[0.5, 0.01, 0.05, 0.075, 0.1, 0.125, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]): 
        """ provides quantile regression predictions for all the given list
            of quantiles
        """
        df = pd.DataFrame({"ground-truth": self.y})
        for q in quantiles:
            model = LGBMRegressor(objective = 'quantile', alpha = q, random_state=42)            
            if q == 0.5:
                scores = self.cv_score_model(model)
                best_model = model
                df["best_prediction"] = self.pred_test
                df["abs_percentage_error"] = round(100*(np.absolute(self.pred_test - self.y)/self.y), 1)
                df["ratio"] = df["best_prediction"] / df["ground-truth"]
                # report best predictions
                print("Best quantile predictions:")
                [print('\t', key,':', val) for key, val in scores.items()]
                if self.online_run:
                    [self.run.log(f"{key}:", val) for key, val in scores.items()]  
            else:
                _ = self.cv_score_model(model)
                df[f"{q}_pred"] = self.pred_test
                model = LGBMRegressor(objective = 'quantile', alpha = 1-q, random_state=42)            
                _ = self.cv_score_model(model)
                df[f"{1-q}_pred"] = self.pred_test 
                df[f"{q}-{1-q}_interval"] = df[f"{1-q}_pred"] - df[f"{q}_pred"]
                
                # postprocessing of quantile regression results
                if postprocessing:
                    df = self.postprocessing_quantile_regression(df, f"{q}_pred", f"{1-q}_pred", f"{q}-{1-q}_interval", threshold_interval)

                # evaluate result
                df[f"{q}-{1-q}_evaluation"] = np.where(((df[f"{q}_pred"] <= df["ground-truth"]) & (df[f"{1-q}_pred"] >= df["ground-truth"])), "Good", "Bad")
                print("\nQuantiles:", q, 1-q)
                print(round(df[f"{q}-{1-q}_interval"].mean(), 2), "is the average of quantile range interval")
                df_evaluation = df.groupby(f"{q}-{1-q}_evaluation").agg({'abs_percentage_error': 'mean', 'ground-truth': 'count'})
                df_evaluation["percentage"] = df_evaluation["ground-truth"].apply(lambda x: "{0:.1%}".format(x/df_evaluation["ground-truth"].sum())) 
                print(df_evaluation)
        # set best 
        self.model = best_model   
        self.pred_test = df["best_prediction"] 
        return df
        
    def quantile_regression_low_high_interval(self, threshold_interval, quantile): 
        """ provides quantile regression predictions for all the given list
            of quantiles
        """
        df = pd.DataFrame({"ground-truth": self.y})

        # for quantile = quantile
        model = LGBMRegressor(objective = 'quantile', alpha = quantile, random_state=42)            
        _ = self.cv_score_model(model)
        df["lowest_prediction"] = self.pred_test.round()
        model = LGBMRegressor(objective = 'quantile', alpha = 1-quantile, random_state=42)            
        _ = self.cv_score_model(model)
        df["highest_prediction"] = self.pred_test.round() 
        df["interval"] = df["highest_prediction"] - df["lowest_prediction"]
  
        # for quantile = 0.5
        model = LGBMRegressor(objective = 'quantile', alpha = 0.5, random_state=42)            
        scores = self.cv_score_model(model)
        self.model = model   
        df["best_prediction"] = self.pred_test.round()
        df["abs_percentage_error"] = round(100*(np.absolute(self.pred_test - self.y)/self.y), 1)
        df["ratio"] = df["best_prediction"] / df["ground-truth"]
        
        # postprocessing of quantile regression results
        df = self.postprocessing_quantile_regression(df, "lowest_prediction", "highest_prediction", "interval", threshold_interval)

        # report best predictions
        print("Best quantile predictions:")
        [print('\t', key,':', val) for key, val in scores.items()]
        if self.online_run:
            [self.run.log(f"{key}:", val) for key, val in scores.items()]  

        # evaluate result
        df["evaluation"] = np.where(((df["lowest_prediction"] <= df["ground-truth"]) & (df["highest_prediction"] >= df["ground-truth"])), "Good", "Bad")
        print("\nQuantiles:", quantile, 1-quantile)
        print(round(df["interval"].mean(), 2), "is the average of quantile range interval")
        df_evaluation = df.groupby("evaluation").agg({'abs_percentage_error': 'mean', 'ground-truth': 'count'})
        df_evaluation["percentage"] = df_evaluation["ground-truth"].apply(lambda x: "{0:.1%}".format(x/df_evaluation["ground-truth"].sum())) 
        print(df_evaluation)

        # set best   
        self.pred_test = df["best_prediction"] 
        return df

    def postprocessing_quantile_regression(self, results, lowest_col, highest_col, interval_col, threshold_interval): 
        """ quantile regression postprocessing """       
        # conditions to change interval predictions 
        condition1 = (results[lowest_col] > results[highest_col])
        condition2 = (results[lowest_col] > results["best_prediction"]) | (results["best_prediction"] > results[highest_col]) 
        condition3 = results[interval_col] <= threshold_interval
        cond = condition1 | condition2 | condition3
        results.loc[cond, lowest_col] = results.loc[cond, "best_prediction"] - threshold_interval/2
        results.loc[cond, highest_col] = results.loc[cond, "best_prediction"] + threshold_interval/2
        results[interval_col] = results[highest_col] - results[lowest_col]
        return results
                    

### AUXILIARY FUNCTIONS ###    
def evaluate(x):
    """ evaluation intervals of the residual ratio between
        two continious features
    """
    if (x >= 50):
        value = "off"
    if (x <= 50):
        value = "50%"
    if (x <= 25):
        value = "25%"
    if (x <= 15):
        value = "15%"
    if (x <= 10):
        value = "10%"
    if (x <= 5):
        value = "5%"
    return value

def regression_metrics(y_test, pred_test, model_name=""):
    """ show metrics for selected two continuous columns to be
        compared with each other 
    """   
    return {
        'model' : model_name,
        'R2': round(r2_score(y_test, pred_test), 3),
        'MAE': round(mean_absolute_error(y_test, pred_test), 3),
        'MAPE': round(mean_absolute_percentage_error(y_test, pred_test), 3),
        'RMSE': round(mean_squared_error(y_test, pred_test, squared=False), 3),
        'sample_size': len(y_test),    
    }

def extract_model_name(model):
    model_name = str(type(model)).split(".")[-1]
    return "".join([char for char in model_name if char not in string.punctuation])